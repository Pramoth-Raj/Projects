import os
from groq import Groq

def get_response_from_LLM(prompt, api_key): 
    """
    Sends a prompt to the Groq LLM and returns the model's response.

    Args:
        prompt (str): The input text for the LLM.
        api_key (str): API key for authenticating with the Groq service.

    Returns:
        str: The response generated by the LLM.
    """

    groq_api_key = api_key

    client = Groq(
        # This is the default and can be omitted
        api_key=groq_api_key,
    )

    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": prompt,
            }
        ],
        model="llama-3.3-70b-versatile",
    )

    return chat_completion.choices[0].message.content

def get_rationale_from_LLM(abstract, conference, api_key, n_words=100):
    """
    Generates a rationale explaining why a research paper is best suited for a specific conference.

    This function constructs a prompt that asks the LLM to justify the classification of a research paper 
    based on its abstract. The response is constrained to a specified word limit.

    Args:
        abstract (str): Abstract of the research paper.
        conference (str): Predicted conference for the paper.
        api_key (str): API key for querying the LLM.
        n_words (int, optional): Maximum number of words in the rationale. Defaults to 100.

    Returns:
        str: The generated rationale explaining the conference suitability.
    """

    prompt = f'Why is the research paper with abstract: {abstract} best suited to be published in {conference} conference in less than {n_words} words'
    rationale = get_response_from_LLM(prompt, api_key)
    return rationale