{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final CSV creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import statistics\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_coherence(s, tokenizer, model):\n",
    "    sentence1 = s\n",
    "    \n",
    "    tokens1 = tokenizer(sentence1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    outputs1 = model(**tokens1)\n",
    "\n",
    "    # cls_embedding1 = outputs1.last_hidden_state[:, 0, :]  # Shape: (1, hidden_size)\n",
    "\n",
    "    # Extract token embeddings (excluding [CLS] and [SEP])\n",
    "    token_embeddings1 = outputs1.last_hidden_state.squeeze(0)[1:-1]\n",
    "\n",
    "    # Compute pairwise cosine similarity for tokens\n",
    "    similarity_matrix1 = cosine_similarity(token_embeddings1.detach().numpy())\n",
    "\n",
    "    coherence_score1 = similarity_matrix1.mean()\n",
    "\n",
    "    return coherence_score1\n",
    "\n",
    "def get_coherence_list(sent):    \n",
    "    path1_coherence = []\n",
    "    for i in sent:\n",
    "        path1_coherence.append(get_coherence(i, tokenizer, model))\n",
    "    return path1_coherence\n",
    "\n",
    "def get_sent_len_list(sent):\n",
    "    return [len(i) for i in sent]\n",
    "\n",
    "def extract_all_from_pdf(pdf_path, include_coherence=True):\n",
    "    # Load the PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page\n",
    "    for page in reader.pages:\n",
    "        full_text += page.extract_text() + \" \"\n",
    "\n",
    "    # Define patterns\n",
    "    numerical_pattern = r'[0-9]'\n",
    "    math_pattern = r'[+\\-*/=^%()]'\n",
    "    math_pattern = r'[σ∑∫π√∞Δθλ+\\-=*/^<>%∂µˆΓαγδθλϵ(){}]'\n",
    "    \n",
    "    # Calculate character counts\n",
    "    total_characters = len(full_text)  # Total characters, including spaces and newlines\n",
    "    numerical_count = len(re.findall(numerical_pattern, full_text))  # Count numerical characters\n",
    "    math_count = len(re.findall(math_pattern, full_text))  # Count mathematical characters\n",
    "\n",
    "    # Basic cleaning to remove headings, equations, and unnecessary content\n",
    "    cleaned_text = re.sub(r\"(\\n|\\\\n)+\", \" \", full_text)  # Remove newlines\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s.,!?-]\", \"\", cleaned_text)  # Remove special characters\n",
    "    cleaned_text = re.sub(r\"\\b[A-Z]{2,}\\b\", \"\", cleaned_text)  # Remove headings (all-uppercase words)\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "    # Filter out equations (e.g., containing \"=\" or numbers with operators)\n",
    "    sentences = [\n",
    "        sentence.strip()\n",
    "        for sentence in sentences\n",
    "        if not re.search(r\"[=+\\-*/^]\", sentence) and len(re.findall(r\"\\d\", sentence)) < len(sentence.split()) // 2\n",
    "    ]\n",
    "\n",
    "    if include_coherence:\n",
    "        coherence = statistics.mean(get_coherence_list(sentences))\n",
    "\n",
    "    sent_len = statistics.mean(get_sent_len_list(sentences))\n",
    "\n",
    "    if include_coherence:\n",
    "        return sent_len, (math_count+numerical_count)/total_characters, coherence\n",
    "    else:\n",
    "        return sent_len, (math_count+numerical_count)/total_characters\n",
    "    \n",
    "def classify_pdf(pdf_path, length_range=[0,160], density_range=[0,1.8], coherence_range=[0.40, 1], include_coherence=True):\n",
    "    if include_coherence:\n",
    "        sentence_length, density, coherence = extract_all_from_pdf(pdf_path, include_coherence=include_coherence)\n",
    "        if (length_range[0]<=sentence_length<=length_range[1]) and (density_range[0]<=density<=density_range[1]) and (coherence_range[0]<=coherence<=coherence_range[1]):\n",
    "            return True\n",
    "        return False \n",
    "    else:\n",
    "        sentence_length, density = extract_all_from_pdf(pdf_path, include_coherence=False)\n",
    "        if (length_range[0]<=sentence_length<=length_range[1]) and (density_range[0]<=density<=density_range[1]):\n",
    "            return True\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_folder = \"Data/Papers\"\n",
    "file_names = []\n",
    "publishable = []\n",
    "for file_name in os.listdir(root_folder):\n",
    "    file_path = os.path.join(root_folder, file_name)\n",
    "    print(f\"file_name: {file_name}\")\n",
    "    \n",
    "    # Check if the file is a PDF\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        # Replace with your processing logic\n",
    "        file_names.append(file_name)\n",
    "        publishable.append(classify_pdf(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Publishable': [int(i) for i in publishable]}, index=[i[:-4] for i in file_names])\n",
    "\n",
    "df.index.name = 'Paper ID'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def find_n_closest_vectors(training_vectors, new_vector, n=1):\n",
    "    \"\"\"\n",
    "    Find the indices of the n closest vectors to the new vector among the training vectors.\n",
    "    \n",
    "    Args:\n",
    "        training_vectors (list or np.ndarray): List or array of training vectors.\n",
    "        new_vector (np.ndarray): The new vector to compare.\n",
    "        n (int): Number of closest vectors to find.\n",
    "    \n",
    "    Returns:\n",
    "        list: Indices of the n closest vectors in the training vectors.\n",
    "    \"\"\"\n",
    "    training_vectors = np.array(training_vectors)  # Ensure it's a NumPy array\n",
    "    new_vector = np.array(new_vector).reshape(1, -1)  # Reshape to match dimensions\n",
    "    similarities = cosine_similarity(training_vectors, new_vector).flatten()  # Compute cosine similarities\n",
    "    closest_indices = np.argsort(similarities)[-n:][::-1]  # Get indices of n highest similarities in descending order\n",
    "    return closest_indices.tolist()\n",
    "\n",
    "\n",
    "\n",
    "def most_frequent_element(lst):\n",
    "    \"\"\"\n",
    "    Find the element with the highest frequency of occurrence in a list.\n",
    "    \n",
    "    Args:\n",
    "        lst (list): Input list of elements.\n",
    "    \n",
    "    Returns:\n",
    "        The element with the highest frequency.\n",
    "    \"\"\"\n",
    "    if not lst:\n",
    "        return None  # Handle empty list case\n",
    "    \n",
    "    counter = Counter(lst)  # Count the frequency of each element\n",
    "    most_common_element = counter.most_common(1)[0][0]  # Get the element with the highest frequency\n",
    "    return most_common_element\n",
    "\n",
    "def extract_abstract(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the title and abstract from a PDF.\n",
    "    Title: Text from start to '\\nAbstract\\n'.\n",
    "    Abstract: Text between '\\nAbstract\\n' and '1 Introduction'.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract_start = text.find(\"\\nAbstract\\n\") + len(\"\\nAbstract\\n\")\n",
    "    abstract_end = text.find(\"1 Introduction\")\n",
    "    abstract = text[abstract_start:abstract_end].strip() if abstract_start != -1 and abstract_end != -1 else \"\"\n",
    "\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_response_from_LLM(prompt, api_key): #this function to be filled in with code that would return the response of the LLM for the input prompt\n",
    "    groq_api_key = api_key\n",
    "\n",
    "    client = Groq(\n",
    "        # This is the default and can be omitted\n",
    "        api_key=groq_api_key,\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def get_rationale_from_LLM(abstract, conference, api_key, n_words=100):\n",
    "    prompt = f'Why is the research paper with abstract: {abstract} best suited to be published in {conference} conference in less than {n_words} words'\n",
    "    rationale = get_response_from_LLM(prompt, api_key)\n",
    "    return rationale\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def predict_conference_and_get_rationale(pdf_path, api_key): # this is the final funciton that gives the predicted conference of the research paper pdf\n",
    "    titles_vector_base = np.load('KNN_train_data.npy') # Vector store can be used here\n",
    "    titles_vector_labels = np.load('KNN_train_labels.npy')\n",
    "    class_label = ['CVPR', 'EMNLP', 'KDD', 'NeurIPS', 'TMLR']\n",
    "    abstract = extract_abstract(pdf_path)\n",
    "    model_name = 'all-mpnet-base-v2'\n",
    "    model = SentenceTransformer(model_name)\n",
    "    abstract_embed = model.encode(abstract)\n",
    "    index = find_n_closest_vectors(titles_vector_base, abstract_embed, n=21)\n",
    "    l = [titles_vector_labels[i] for i in index]\n",
    "    conference_class = most_frequent_element(l)\n",
    "    conference = class_label[conference_class]\n",
    "    rationale = get_rationale_from_LLM(abstract, conference, api_key)\n",
    "    return conference, rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_groq_api_key = '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'\n",
    "pdf_path = r\"Data\\Reference\\Publishable\\TMLR\\R015.pdf\"\n",
    "conf, ration = predict_conference_and_get_rationale(pdf_path, sample_groq_api_key)\n",
    "print(conf)\n",
    "print(ration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for all test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though not needed for all files, only the ones classified as 1 in Publishable column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_folder = \"Data/Papers\"\n",
    "sample_groq_api_key = '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'\n",
    "file_names = []\n",
    "conferences = []\n",
    "rationales = []\n",
    "for file_name in os.listdir(root_folder):\n",
    "    file_path = os.path.join(root_folder, file_name)\n",
    "    print(f\"file_name: {file_name}\")\n",
    "    \n",
    "    # Check if the file is a PDF\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        # Replace with your processing logic\n",
    "        file_names.append(file_name)\n",
    "        conference, rationale = predict_conference_and_get_rationale(file_path, sample_groq_api_key)\n",
    "        conferences.append(conference)\n",
    "        rationales.append(rationale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns and create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Conference'] = conferences\n",
    "df['Rationale'] = rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Publishable'] == 0, ['Conference', 'Rationale']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
